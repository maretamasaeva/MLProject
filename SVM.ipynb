{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c665727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using linear support vector machine to predict speaker from text!\n",
    "# the internet says it is one of the best algorithms for text classification, so that's why i'll be using it \n",
    "# also, maybe only predicting two labels was making it difficult for the naive bayes. trying speakers here because that's a lot more labels (and it is more fun!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d204bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn \n",
    "print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c49f70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('The-Office-Lines-V4.csv')\n",
    "df= df.drop('Unnamed: 6', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831f17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender(name):\n",
    "    if name == 'Michael' or name == 'Jim' or name == 'Dwight' or name == 'Ryan' or name == 'Stanley' or name == 'Kevin' or name == 'Oscar' or name == 'Toby' or name == 'Creed' or name == 'Andy' or name == 'Roy' or name == 'Darryl' or name == 'Gabe' or name == 'Robert California' or name == 'Robert' or name == 'Clark' or name == 'Pete':\n",
    "        return 0\n",
    "    elif name == 'Pam' or name == 'Phyllis' or name == 'Angela' or name == 'Kelly' or name == 'Meredith' or name == 'Jan' or name == 'Erin' or name == 'Nellie':\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc05ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gender\"] = df[\"speaker\"].map(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41606377",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad5bd7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'] = df['gender'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fd596cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_of_words'] = df['line'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2868da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['speaker_id'] = pd.factorize(df.speaker)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bc073d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>title</th>\n",
       "      <th>scene</th>\n",
       "      <th>speaker</th>\n",
       "      <th>line</th>\n",
       "      <th>gender</th>\n",
       "      <th>number_of_words</th>\n",
       "      <th>speaker_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11616</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>Cocktails</td>\n",
       "      <td>2012</td>\n",
       "      <td>Jan</td>\n",
       "      <td>Shut up.</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       season  episode      title  scene speaker      line  gender  \\\n",
       "11616       3       18  Cocktails   2012     Jan  Shut up.       1   \n",
       "\n",
       "       number_of_words  speaker_id  \n",
       "11616                2           4  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fda83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2b6e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['line']\n",
    "y = df['speaker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4248d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=42, test_size=.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cea48b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31934,), (31934,), (15730,), (15730,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7dc1288",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "sgd = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9f099cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(vect, tfidf, sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed30b08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                ('tfidftransformer', TfidfTransformer()),\n",
       "                ('sgdclassifier', SGDClassifier())])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1279cd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28463                                   Okay.  You owe me.\n",
       "10883                                               Sally.\n",
       "51277                        No. She needs her rest again.\n",
       "4371      What do you do with a drunken sailor? What do...\n",
       "27832                                       And oh, Jim...\n",
       "                               ...                        \n",
       "12558                                                  OK.\n",
       "51152                   All right. I think that went well.\n",
       "43751                 There is one problem with this plan.\n",
       "910                               It's just office pranks.\n",
       "17535                 Hey Pam, I really like your glasses.\n",
       "Name: line, Length: 31934, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d395ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2396c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tags = ['Michael', 'Jim', 'Pam', 'Dwight', 'Jan', 'Phyllis', 'Stanley',\n",
    "       'Oscar', 'Angela', 'Kevin', 'Ryan', 'Roy', 'Toby', 'Kelly',\n",
    "       'Meredith', 'Darryl', 'Creed', 'Andy', 'Pete', 'Erin', 'Gabe',\n",
    "       'Clark', 'Robert', 'Robert California', 'Nellie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64ec7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37291bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.23808010171646535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maret\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          Michael       0.16      0.17      0.17      1223\n",
      "              Jim       0.17      0.08      0.11       496\n",
      "              Pam       0.01      0.01      0.01        97\n",
      "           Dwight       0.08      0.04      0.06       135\n",
      "              Jan       0.18      0.12      0.15       361\n",
      "          Phyllis       0.26      0.33      0.29      2232\n",
      "          Stanley       0.11      0.07      0.09       469\n",
      "            Oscar       0.07      0.06      0.07       139\n",
      "           Angela       0.10      0.05      0.07       268\n",
      "            Kevin       0.22      0.21      0.21      2037\n",
      "             Ryan       0.09      0.05      0.07       284\n",
      "              Roy       0.09      0.07      0.08       530\n",
      "             Toby       0.06      0.04      0.05       184\n",
      "            Kelly       0.33      0.52      0.40      3545\n",
      "         Meredith       0.16      0.06      0.09       162\n",
      "           Darryl       0.10      0.05      0.06       448\n",
      "            Creed       0.19      0.13      0.16      1675\n",
      "             Andy       0.09      0.03      0.04        74\n",
      "             Pete       0.10      0.06      0.07       322\n",
      "             Erin       0.23      0.10      0.14       123\n",
      "             Gabe       0.00      0.00      0.00         4\n",
      "            Clark       0.03      0.02      0.02        65\n",
      "           Robert       0.10      0.04      0.06       385\n",
      "Robert California       0.06      0.03      0.04       218\n",
      "           Nellie       0.09      0.07      0.08       254\n",
      "\n",
      "         accuracy                           0.24     15730\n",
      "        macro avg       0.12      0.10      0.10     15730\n",
      "     weighted avg       0.21      0.24      0.21     15730\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maret\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\maret\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92f252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff7fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e88bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165c424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9f04139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to do it like in class, but SVC() causes problems (in class we were taking a numeric value and predicting a binary value; here, it is text to numeric(speaker_id))\n",
    "# it tries to convert the input lines to floats, which isnt possible. \n",
    "\n",
    "#using tfidftransformer should fix this; transform text into numeric data\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.svm import LinearSVC, SVC \n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "# imputer = KNNImputer()\n",
    "vect = CountVectorizer()\n",
    "scaler = MaxAbsScaler()\n",
    "tfidf = TfidfTransformer()\n",
    "# scaler = MinMaxScaler()\n",
    "svm1 = LinearSVC()\n",
    "svm2 = SVC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3d07883",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = make_pipeline(vect, scaler, tfidf, svm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "827f18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = make_pipeline(vect, scaler, tfidf, svm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "befad231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': True,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'loss': 'squared_hinge',\n",
       " 'max_iter': 1000,\n",
       " 'multi_class': 'ovr',\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8423a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21bdcad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['line']\n",
    "y = df['speaker_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faeb7260",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 0.0690114604166341, Std 0.006894584452084698\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipe1, X, y, cv=10, scoring='f1_macro') # only measures the harmonic mean of the positive class\n",
    "print(f\"Mean {scores.mean()}, Std {scores.std()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96703e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont have to run this (takes too long idk why) bc im checking the best kernel in the next cell\n",
    "# scores = cross_val_score(pipe2, X, y, cv=10, scoring='f1_macro') # only measures the harmonic mean of the positive class (here: survived)\n",
    "# print(f\"Mean {scores.mean()}, Std {scores.std()}\")\n",
    "\n",
    "\n",
    "# poly and rbf kernels take very long to run (actually they dont run at all) so were sticking to linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b891a27b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernels = ['linear', 'poly'] # poly for polynomial # defining the diff kernels here, you want to see if there is any difference between these three\n",
    "# accuracies= [] \n",
    "\n",
    "# for k in kernels:\n",
    "#     svm = SVC(kernel=k) # initializing your support vector machine\n",
    "#     pipe3 = make_pipeline(vect, tfidf, svm)\n",
    "#     scores = cross_val_score(pipe3, X, y, cv=10, scoring='f1_macro') # doing crossvalidation to see if the model is robust\n",
    "#     accuracies.append(scores.mean())\n",
    "\n",
    "# plt.plot(kernels, accuracies)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # this wont run!! something with rbf and poly kernels, because linear does work on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33ff5256",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                ('maxabsscaler', MaxAbsScaler()),\n",
       "                ('tfidftransformer', TfidfTransformer()),\n",
       "                ('linearsvc', LinearSVC())])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21ca3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = pipe1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e225c70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          Michael       0.20      0.13      0.16      1223\n",
      "              Jim       0.17      0.08      0.11       496\n",
      "              Pam       0.04      0.01      0.02        97\n",
      "           Dwight       0.15      0.04      0.07       135\n",
      "              Jan       0.17      0.10      0.12       361\n",
      "          Phyllis       0.26      0.27      0.27      2232\n",
      "          Stanley       0.14      0.07      0.10       469\n",
      "            Oscar       0.11      0.04      0.05       139\n",
      "           Angela       0.08      0.03      0.04       268\n",
      "            Kevin       0.21      0.27      0.24      2037\n",
      "             Ryan       0.15      0.05      0.08       284\n",
      "              Roy       0.13      0.08      0.09       530\n",
      "             Toby       0.09      0.04      0.05       184\n",
      "            Kelly       0.29      0.53      0.38      3545\n",
      "         Meredith       0.13      0.06      0.08       162\n",
      "           Darryl       0.11      0.06      0.08       448\n",
      "            Creed       0.19      0.17      0.18      1675\n",
      "             Andy       0.04      0.01      0.02        74\n",
      "             Pete       0.12      0.04      0.06       322\n",
      "             Erin       0.17      0.07      0.09       123\n",
      "             Gabe       0.00      0.00      0.00         4\n",
      "            Clark       0.07      0.03      0.04        65\n",
      "           Robert       0.11      0.04      0.06       385\n",
      "Robert California       0.06      0.02      0.03       218\n",
      "           Nellie       0.15      0.05      0.08       254\n",
      "\n",
      "         accuracy                           0.24     15730\n",
      "        macro avg       0.13      0.09      0.10     15730\n",
      "     weighted avg       0.21      0.24      0.21     15730\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_test_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb7bf46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f12506fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'linearsvc__C': [0.1, 1, 10, 100, 1000],\n",
    "               'countvectorizer__ngram_range': [(1,1),(1,2),(2,2)], \n",
    "               'tfidftransformer__use_idf': (True, False),\n",
    "               'tfidftransformer__norm': ('l1', 'l2')\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "029aa7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe1, param_grid, cv=3, refit = True, verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "823c3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid2 = {'C': [0.1, 1, 10, 100, 1000],\n",
    "#              'verbose': [5],\n",
    "#              'loss': ('hinge', 'squared_hinge'),\n",
    "#              'multi_class': ('ovr', 'crammer_singer')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59e1e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs2 = GridSearchCV(svm1, param_grid2, cv=3, scoring='accuracy', n_jobs=-1, refit = True, verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ade06da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental_pipe = make_pipeline(vect, tfidf, gs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a9102b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "569e393b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this takes hours and doesnt run; maybe gridsearch enters infinite loop; try looking for best parameters manually\n",
    "# (kernel, C, gamma)\n",
    "# gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a781560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying rbf kernel manually, without gridsearch\n",
    "# vect = CountVectorizer()\n",
    "# scaler = MaxAbsScaler()\n",
    "# tfidf = TfidfTransformer()\n",
    "# svm2 = SVC()\n",
    "\n",
    "# pipe2 = make_pipeline(vect, scaler, tfidf, svm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d51c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also doesnt run\n",
    "# pipe2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3885f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_pred = pipe1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "666d9261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, y_test_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a314913d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb78330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCLUSION : gridsearch causes a lot of trouble, and wont let anything run\n",
    "# trying other kernels manually also doesnt work\n",
    "# however, i tried running all this in a google colab notebook, and it returned the highest score for the linear kernel\n",
    "# so we will stick to the linear kernel!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
